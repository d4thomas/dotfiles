#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.9"
# dependencies = []
# ///

import sys
import argparse
import uuid
import re
import json
import zipfile
import tempfile
import shutil
from pathlib import Path
from datetime import datetime
from xml.etree import ElementTree as ET

try:
    import urllib.request
    import urllib.parse
except ImportError:
    pass


def parse_manga_title(filename):
    """Extract base title from filename by removing volume/chapter numbers"""
    name = filename.replace('.epub', '').replace('.EPUB', '')

    patterns = [
        r'\s*[-_]\s*v(?:ol(?:ume)?\.?)?\s*\d+.*$',
        r'\s*v(?:ol(?:ume)?\.?)?\s*\d+.*$',
        r'\s*[-_]\s*\d+.*$',
        r'\s*#\d+.*$',
        r'\s*\(\d+\).*$',
        r'\s*\[\d+\].*$',
    ]

    for pattern in patterns:
        name = re.sub(pattern, '', name, flags=re.IGNORECASE)

    name = re.sub(r'\s+', ' ', name).strip()

    return name


def fetch_anilist_metadata(title):
    """Fetch metadata from AniList (manga/anime database)"""
    try:
        query = '''
        query ($search: String) {
          Media(search: $search, type: MANGA) {
            staff {
              edges {
                role
                node {
                  name {
                    full
                  }
                }
              }
            }
            startDate {
              year
              month
              day
            }
            endDate {
              year
              month
              day
            }
          }
        }
        '''

        variables = {'search': title}

        data = json.dumps({
            'query': query,
            'variables': variables
        }).encode('utf-8')

        req = urllib.request.Request(
            'https://graphql.anilist.co',
            data=data,
            headers={'Content-Type': 'application/json'}
        )

        with urllib.request.urlopen(req, timeout=5) as response:
            result = json.loads(response.read().decode('utf-8'))

            if 'data' in result and result['data']['Media']:
                media = result['data']['Media']

                author = None
                if 'staff' in media and media['staff']['edges']:
                    for edge in media['staff']['edges']:
                        role = edge['role']
                        if any(keyword in role for keyword in ['Story & Art', 'Story', 'Original Creator']):
                            author = edge['node']['name']['full']
                            break

                # Format dates
                start_date = None
                if media.get('startDate') and media['startDate'].get('year'):
                    start_date = f"{media['startDate']['year']}"

                return {
                    'author': author,
                    'publication_date': start_date,
                    'source': 'AniList'
                }

        return None
    except Exception as e:
        print(f"  AniList: {e}")
        return None


def fetch_google_books_metadata(title):
    """Fetch metadata from Google Books API"""
    try:
        query = urllib.parse.quote(title)
        url = f"https://www.googleapis.com/books/v1/volumes?q={query}&maxResults=1"

        req = urllib.request.Request(url)
        with urllib.request.urlopen(req, timeout=5) as response:
            result = json.loads(response.read().decode('utf-8'))

            if result.get('totalItems', 0) > 0:
                book = result['items'][0]['volumeInfo']

                author = None
                if book.get('authors'):
                    author = ', '.join(book['authors'])

                pub_date = None
                if book.get('publishedDate'):
                    # Extract just the year
                    pub_date = book['publishedDate'][:4]

                return {
                    'author': author,
                    'publication_date': pub_date,
                    'source': 'Google Books'
                }

        return None
    except Exception as e:
        print(f"  Google Books: {e}")
        return None


def fetch_openlibrary_metadata(title):
    """Fetch metadata from Open Library API"""
    try:
        query = urllib.parse.quote(title)
        url = f"https://openlibrary.org/search.json?title={query}&limit=1"

        req = urllib.request.Request(url)
        with urllib.request.urlopen(req, timeout=5) as response:
            result = json.loads(response.read().decode('utf-8'))

            if result.get('numFound', 0) > 0:
                book = result['docs'][0]

                author = None
                if book.get('author_name'):
                    author = ', '.join(book['author_name'][:2])  # Limit to first 2 authors

                pub_date = None
                if book.get('first_publish_year'):
                    pub_date = str(book['first_publish_year'])

                return {
                    'author': author,
                    'publication_date': pub_date,
                    'source': 'Open Library'
                }

        return None
    except Exception as e:
        print(f"  Open Library: {e}")
        return None


def fetch_mangadex_metadata(title):
    """Fetch metadata from MangaDex API"""
    try:
        query = urllib.parse.quote(title)
        url = f"https://api.mangadex.org/manga?title={query}&limit=1&includes[]=author&includes[]=artist"

        req = urllib.request.Request(url)
        with urllib.request.urlopen(req, timeout=5) as response:
            result = json.loads(response.read().decode('utf-8'))

            if result.get('data') and len(result['data']) > 0:
                manga = result['data'][0]

                # Extract author from relationships
                author = None
                for rel in manga.get('relationships', []):
                    if rel['type'] in ['author', 'artist']:
                        author_name = rel.get('attributes', {}).get('name')
                        if author_name:
                            author = author_name
                            break

                # Get publication year
                pub_date = None
                attrs = manga.get('attributes', {})
                if attrs.get('year'):
                    pub_date = str(attrs['year'])

                return {
                    'author': author,
                    'publication_date': pub_date,
                    'source': 'MangaDex'
                }

        return None
    except Exception as e:
        print(f"  MangaDex: {e}")
        return None


def fetch_metadata(title):
    """
    Fetch metadata from multiple sources with fallback.
    Tries sources in order until finding good metadata.
    """
    print(f"Searching for metadata: '{title}'")

    # Try each source in order of preference
    sources = [
        fetch_anilist_metadata,      # Best for manga/anime
        fetch_mangadex_metadata,      # Manga-specific
        fetch_google_books_metadata,  # General books
        fetch_openlibrary_metadata,   # Comprehensive fallback
    ]

    for fetch_func in sources:
        metadata = fetch_func(title)
        if metadata and metadata.get('author'):  # Only accept if we found an author
            source = metadata.get('source', 'Unknown')
            print(f"  ✓ Found metadata from {source}")
            if metadata.get('author'):
                print(f"    Author: {metadata['author']}")
            if metadata.get('publication_date'):
                print(f"    Published: {metadata['publication_date']}")
            return metadata

    print(f"  ✗ No metadata found from any source")
    return None


def update_content_opf(opf_content, metadata, title):
    """Update the content.opf XML with new metadata"""
    # Parse the XML
    ET.register_namespace('', 'http://www.idpf.org/2007/opf')
    ET.register_namespace('dc', 'http://purl.org/dc/elements/1.1/')

    root = ET.fromstring(opf_content)

    # Define namespaces
    ns = {
        'opf': 'http://www.idpf.org/2007/opf',
        'dc': 'http://purl.org/dc/elements/1.1/'
    }

    # Find the metadata section
    metadata_elem = root.find('opf:metadata', ns)
    if metadata_elem is None:
        metadata_elem = root.find('metadata')

    if metadata_elem is None:
        print("  Warning: Could not find metadata section in content.opf")
        return opf_content

    # Remove old title, author, date, and subject elements
    for elem in list(metadata_elem):
        tag = elem.tag.split('}')[-1]  # Remove namespace
        if tag in ['title', 'creator', 'date', 'subject']:
            metadata_elem.remove(elem)

    # Add new metadata
    dc_ns = '{http://purl.org/dc/elements/1.1/}'

    # Add title (from filename without extension)
    title_elem = ET.Element(f'{dc_ns}title')
    title_elem.text = title
    metadata_elem.append(title_elem)

    if metadata and metadata.get('author'):
        creator = ET.Element(f'{dc_ns}creator')
        creator.text = metadata['author']
        metadata_elem.append(creator)

    if metadata and metadata.get('publication_date'):
        date = ET.Element(f'{dc_ns}date')
        date.text = metadata['publication_date']
        metadata_elem.append(date)
    else:
        # Use current timestamp if no publication date
        date = ET.Element(f'{dc_ns}date')
        date.text = datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ")
        metadata_elem.append(date)

    # Convert back to string with proper formatting
    xml_str = ET.tostring(root, encoding='unicode', method='xml')

    # Add XML declaration
    if not xml_str.startswith('<?xml'):
        xml_str = '<?xml version="1.0" encoding="UTF-8" standalone="yes"?>\n' + xml_str

    return xml_str


def update_epub_metadata(epub_path, metadata=None, title=None, create_backup=False):
    """Update metadata in an EPUB file"""
    epub_path = Path(epub_path)

    if not epub_path.exists():
        print(f"Error: EPUB file not found: {epub_path}")
        return False

    if title is None:
        title = epub_path.stem

    print(f"\nProcessing: {epub_path.name}")

    # If no metadata provided, fetch it
    if metadata is None:
        series_title = parse_manga_title(epub_path.name)
        metadata = fetch_metadata(series_title)

    # Create temporary directory for extraction
    temp_dir = Path(tempfile.mkdtemp())

    try:
        # Extract EPUB (it's just a zip file)
        print("Extracting EPUB...")
        with zipfile.ZipFile(epub_path, 'r') as zip_ref:
            zip_ref.extractall(temp_dir)

        # Find OPF file (could be content.opf, standard.opf, package.opf, etc.)
        opf_path = None
        for possible_path in [
            temp_dir / 'OEBPS' / 'content.opf',
            temp_dir / 'OEBPS' / 'standard.opf',
            temp_dir / 'OEBPS' / 'package.opf',
            temp_dir / 'OPS' / 'content.opf',
            temp_dir / 'OPS' / 'standard.opf',
            temp_dir / 'OPS' / 'package.opf',
            temp_dir / 'content.opf',
        ]:
            if possible_path.exists():
                opf_path = possible_path
                break

        # Search recursively if not found
        if opf_path is None:
            for opf_file in temp_dir.rglob('*.opf'):
                opf_path = opf_file
                break

        if opf_path is None:
            print("  Error: Could not find content.opf in EPUB")
            return False

        print(f"  Updating {opf_path.relative_to(temp_dir)}...")

        # Read and update content.opf
        with open(opf_path, 'r', encoding='utf-8') as f:
            opf_content = f.read()

        updated_opf = update_content_opf(opf_content, metadata, title)

        # Write updated content.opf
        with open(opf_path, 'w', encoding='utf-8') as f:
            f.write(updated_opf)

        # Create backup of original if requested
        if create_backup:
            backup_path = epub_path.with_suffix('.epub.backup')
            shutil.copy2(epub_path, backup_path)
            print(f"  Backup created: {backup_path.name}")

        # Repackage EPUB
        print("  Repackaging EPUB...")
        with zipfile.ZipFile(epub_path, 'w', zipfile.ZIP_DEFLATED) as epub:
            # mimetype must be first and uncompressed
            mimetype_path = temp_dir / 'mimetype'
            if mimetype_path.exists():
                epub.write(mimetype_path, 'mimetype', compress_type=zipfile.ZIP_STORED)

            # Add all other files
            for file_path in temp_dir.rglob('*'):
                if file_path.is_file() and file_path.name != 'mimetype':
                    arcname = file_path.relative_to(temp_dir)
                    epub.write(file_path, arcname, compress_type=zipfile.ZIP_DEFLATED)

        print(f"  ✓ Successfully updated: {epub_path.name}")
        return True

    except Exception as e:
        print(f"  Error updating EPUB: {e}")
        import traceback
        traceback.print_exc()
        return False

    finally:
        if temp_dir.exists():
            shutil.rmtree(temp_dir)


def main():
    parser = argparse.ArgumentParser(
        description='Update EPUB metadata to match PDFtoEpub format. Fetches author and publication date from online sources.'
    )
    parser.add_argument('epub_files', nargs='*',
                       help='EPUB file(s) to update. If not specified, processes all EPUBs in current directory.')
    parser.add_argument('-t', '--title',
                       help='Override title for metadata search (default: extracted from filename)')
    parser.add_argument('--backup', action='store_true',
                       help='Create backup files (.epub.backup)')

    args = parser.parse_args()

    # Get list of EPUB files to process
    if args.epub_files:
        epub_paths = [Path(f) for f in args.epub_files]
    else:
        epub_paths = list(Path.cwd().glob('*.epub'))
        epub_paths.extend(Path.cwd().glob('*.EPUB'))

    if not epub_paths:
        print("No EPUB files found")
        sys.exit(1)

    print(f"Found {len(epub_paths)} EPUB file(s) to process")

    success_count = 0
    for epub_path in epub_paths:
        print(f"\n{'='*60}")
        title = args.title if args.title and len(epub_paths) == 1 else None
        success = update_epub_metadata(epub_path, title=title, create_backup=args.backup)
        if success:
            success_count += 1
        print(f"{'='*60}")

    print(f"\nComplete: {success_count}/{len(epub_paths)} successful")
    sys.exit(0 if success_count == len(epub_paths) else 1)


if __name__ == '__main__':
    main()
