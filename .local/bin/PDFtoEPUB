#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.9"
# dependencies = [
#     "pillow",
# ]
# ///

import sys
import argparse
import uuid
import subprocess
from pathlib import Path
from datetime import datetime
import zipfile
import shutil
import os
from PIL import Image
import re
import json
import tempfile
import time
import hashlib
from multiprocessing import Pool, cpu_count
try:
    import urllib.request
    import urllib.parse
except ImportError:
    pass



def parse_manga_title(filename):
    name = filename.replace('.pdf', '').replace('.PDF', '')

    patterns = [
        r'\s*[-_]\s*v(?:ol(?:ume)?\.?)?\s*\d+.*$',
        r'\s*v(?:ol(?:ume)?\.?)?\s*\d+.*$',
        r'\s*[-_]\s*\d+.*$',
        r'\s*#\d+.*$',
        r'\s*\(\d+\).*$',
        r'\s*\[\d+\].*$',
    ]

    for pattern in patterns:
        name = re.sub(pattern, '', name, flags=re.IGNORECASE)

    name = re.sub(r'\s+', ' ', name).strip()

    return name


def fetch_anilist_metadata(title):
    """Fetch metadata from AniList (manga/anime database)"""
    try:
        query = '''
        query ($search: String) {
          Media(search: $search, type: MANGA) {
            staff {
              edges {
                role
                node {
                  name {
                    full
                  }
                }
              }
            }
            startDate {
              year
              month
              day
            }
            endDate {
              year
              month
              day
            }
          }
        }
        '''

        variables = {'search': title}

        data = json.dumps({
            'query': query,
            'variables': variables
        }).encode('utf-8')

        req = urllib.request.Request(
            'https://graphql.anilist.co',
            data=data,
            headers={'Content-Type': 'application/json'}
        )

        with urllib.request.urlopen(req, timeout=5) as response:
            result = json.loads(response.read().decode('utf-8'))

            if 'data' in result and result['data']['Media']:
                media = result['data']['Media']

                author = None
                if 'staff' in media and media['staff']['edges']:
                    for edge in media['staff']['edges']:
                        role = edge['role']
                        if any(keyword in role for keyword in ['Story & Art', 'Story', 'Original Creator']):
                            author = edge['node']['name']['full']
                            break

                # Format dates
                start_date = None
                if media.get('startDate') and media['startDate'].get('year'):
                    start_date = f"{media['startDate']['year']}"

                return {
                    'author': author,
                    'publication_date': start_date,
                    'source': 'AniList'
                }

        return None
    except Exception as e:
        print(f"  AniList: {e}")
        return None


def fetch_google_books_metadata(title):
    """Fetch metadata from Google Books API"""
    try:
        query = urllib.parse.quote(title)
        url = f"https://www.googleapis.com/books/v1/volumes?q={query}&maxResults=1"

        req = urllib.request.Request(url)
        with urllib.request.urlopen(req, timeout=5) as response:
            result = json.loads(response.read().decode('utf-8'))

            if result.get('totalItems', 0) > 0:
                book = result['items'][0]['volumeInfo']

                author = None
                if book.get('authors'):
                    author = ', '.join(book['authors'])

                pub_date = None
                if book.get('publishedDate'):
                    # Extract just the year
                    pub_date = book['publishedDate'][:4]

                return {
                    'author': author,
                    'publication_date': pub_date,
                    'source': 'Google Books'
                }

        return None
    except Exception as e:
        print(f"  Google Books: {e}")
        return None


def fetch_openlibrary_metadata(title):
    """Fetch metadata from Open Library API"""
    try:
        query = urllib.parse.quote(title)
        url = f"https://openlibrary.org/search.json?title={query}&limit=1"

        req = urllib.request.Request(url)
        with urllib.request.urlopen(req, timeout=5) as response:
            result = json.loads(response.read().decode('utf-8'))

            if result.get('numFound', 0) > 0:
                book = result['docs'][0]

                author = None
                if book.get('author_name'):
                    author = ', '.join(book['author_name'][:2])  # Limit to first 2 authors

                pub_date = None
                if book.get('first_publish_year'):
                    pub_date = str(book['first_publish_year'])

                return {
                    'author': author,
                    'publication_date': pub_date,
                    'source': 'Open Library'
                }

        return None
    except Exception as e:
        print(f"  Open Library: {e}")
        return None


def fetch_mangadex_metadata(title):
    """Fetch metadata from MangaDex API"""
    try:
        query = urllib.parse.quote(title)
        url = f"https://api.mangadex.org/manga?title={query}&limit=1&includes[]=author&includes[]=artist"

        req = urllib.request.Request(url)
        with urllib.request.urlopen(req, timeout=5) as response:
            result = json.loads(response.read().decode('utf-8'))

            if result.get('data') and len(result['data']) > 0:
                manga = result['data'][0]

                # Extract author from relationships
                author = None
                for rel in manga.get('relationships', []):
                    if rel['type'] in ['author', 'artist']:
                        author_name = rel.get('attributes', {}).get('name')
                        if author_name:
                            author = author_name
                            break

                # Get publication year
                pub_date = None
                attrs = manga.get('attributes', {})
                if attrs.get('year'):
                    pub_date = str(attrs['year'])

                return {
                    'author': author,
                    'publication_date': pub_date,
                    'source': 'MangaDex'
                }

        return None
    except Exception as e:
        print(f"  MangaDex: {e}")
        return None


def fetch_metadata(title):
    """
    Fetch metadata from multiple sources with fallback.
    Tries sources in order until finding good metadata.
    """
    print(f"Searching for metadata: '{title}'")

    # Try each source in order of preference
    sources = [
        fetch_anilist_metadata,      # Best for manga/anime
        fetch_mangadex_metadata,      # Manga-specific
        fetch_google_books_metadata,  # General books
        fetch_openlibrary_metadata,   # Comprehensive fallback
    ]

    for fetch_func in sources:
        metadata = fetch_func(title)
        if metadata and metadata.get('author'):  # Only accept if we found an author
            source = metadata.get('source', 'Unknown')
            print(f"  ✓ Found metadata from {source}")
            if metadata.get('author'):
                print(f"    Author: {metadata['author']}")
            if metadata.get('publication_date'):
                print(f"    Published: {metadata['publication_date']}")
            return metadata

    print(f"  ✗ No metadata found from any source")
    return None


def create_mimetype(output_dir):
    mimetype_path = output_dir / "mimetype"
    with open(mimetype_path, 'w', encoding='utf-8') as f:
        f.write("application/epub+zip")
    return mimetype_path


def create_container_xml(output_dir):
    meta_inf = output_dir / "META-INF"
    meta_inf.mkdir(exist_ok=True)

    container_xml = """<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<container version="1.0" xmlns="urn:oasis:names:tc:opendocument:xmlns:container">
	<rootfiles>
		<rootfile full-path="OEBPS/content.opf" media-type="application/oebps-package+xml" />
	</rootfiles>
</container>
"""

    with open(meta_inf / "container.xml", 'w', encoding='utf-8') as f:
        f.write(container_xml)


def create_css(output_dir):
    css_dir = output_dir / "OEBPS" / "css"
    css_dir.mkdir(parents=True, exist_ok=True)

    css_content = """body, div, dl, dt, dd, h1, h2, h3, h4, h5, h6, p, pre, code, blockquote {
	margin:0;
	padding:0;
	border-width:0;
	text-rendering:optimizeSpeed;
}
body {
	margin:0;
	padding:0;
}
.page-container {
	position:absolute;
	left:0px;
	top:0px;
	width:100%;
	height:100%;
}
img.full-page {
	height:100%;
	width:100%;
	object-fit:contain;
	position:absolute;
	left:0px;
	top:0px;
}
"""

    with open(css_dir / "style.css", 'w', encoding='utf-8') as f:
        f.write(css_content)


def create_xhtml_page(page_num, image_filename, image_width, image_height):
    xhtml = f"""<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
	<head>
		<meta charset="utf-8" />
		<meta name="viewport" content="width={image_width},height={image_height}" />
		<title>Page {page_num}</title>
		<link href="css/style.css" rel="stylesheet" type="text/css" />
	</head>
	<body style="width:{image_width}px;height:{image_height}px;margin:0;padding:0">
		<div class="page-container">
			<img class="full-page" src="image/{image_filename}" alt="Page {page_num}" />
		</div>
	</body>
</html>
"""
    return xhtml


def create_content_opf(output_dir, title, num_pages, image_sizes, metadata=None, reading_direction='rtl'):
    book_id = str(uuid.uuid4())
    timestamp = datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ")

    author = metadata.get('author') if metadata else None
    pub_date = metadata.get('publication_date') if metadata else None

    manifest_pages = []
    for i in range(1, num_pages + 1):
        manifest_pages.append(f'\t\t<item id="page-{i}" href="page-{i}.xhtml" media-type="application/xhtml+xml" />')

    manifest_images = []
    for i in range(1, num_pages + 1):
        properties = ' properties="cover-image"' if i == 1 else ''
        manifest_images.append(f'\t\t<item id="img-{i}" href="image/page-{i}.jpg" media-type="image/jpeg"{properties} />')

    spine_items = []
    for i in range(1, num_pages + 1):
        spine_items.append(f'\t\t<itemref idref="page-{i}" />')

    metadata_lines = [
        f'\t\t<meta name="cover" content="img-1" />',
        f'\t\t<dc:title>{title}</dc:title>',
    ]

    if author:
        metadata_lines.append(f'\t\t<dc:creator>{author}</dc:creator>')

    if pub_date:
        metadata_lines.append(f'\t\t<dc:date>{pub_date}</dc:date>')
    else:
        metadata_lines.append(f'\t\t<dc:date>{timestamp}</dc:date>')

    metadata_lines.extend([
        f'\t\t<dc:language>en</dc:language>',
        f'\t\t<meta property="dcterms:modified">{timestamp}</meta>',
        f'\t\t<dc:identifier id="bookid">urn:uuid:{book_id}</dc:identifier>',
        f'\t\t<meta property="rendition:layout">pre-paginated</meta>',
        f'\t\t<meta property="rendition:spread">auto</meta>',
    ])

    content_opf = f"""<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<package version="3.0" xmlns="http://www.idpf.org/2007/opf" unique-identifier="bookid" prefix="rendition: http://www.idpf.org/vocab/rendition/# ibooks: http://vocabulary.itunes.apple.com/rdf/ibooks/vocabulary-extensions-1.0/">
	<metadata xmlns:dc="http://purl.org/dc/elements/1.1/">
{chr(10).join(metadata_lines)}
	</metadata>
	<manifest>
{chr(10).join(manifest_pages)}
		<item id="nav" href="nav.xhtml" media-type="application/xhtml+xml" properties="nav" />
		<item id="css" href="css/style.css" media-type="text/css" />
{chr(10).join(manifest_images)}
	</manifest>
	<spine page-progression-direction="{reading_direction}">
{chr(10).join(spine_items)}
	</spine>
</package>
"""

    with open(output_dir / "OEBPS" / "content.opf", 'w', encoding='utf-8') as f:
        f.write(content_opf)


def extract_and_process_images(pdf_path, output_dir, left_to_right=False):
    """
    Extracts images from a PDF, splits spreads, and returns image sizes.
    This function is a Python implementation of the logic in extract_manga_pages.sh.
    """
    print("Extracting and processing images...")

    image_dir = output_dir / "OEBPS" / "image"
    image_dir.mkdir(parents=True, exist_ok=True)
    
    temp_extract_dir = Path(tempfile.mkdtemp(prefix="pdf_extract_"))

    try:
        # Extract images using pdfimages
        subprocess.run(
            ['pdfimages', '-j', '-p', str(pdf_path), str(temp_extract_dir / 'page')],
            check=True, capture_output=True
        )

        page_num = 1
        seen_hashes = set()
        image_sizes = []
        
        # Sort extracted files numerically
        extracted_files = sorted(
            [f for f in temp_extract_dir.iterdir() if f.is_file()],
            key=lambda f: int(re.search(r'\d+', f.name).group()) if re.search(r'\d+', f.name) else -1
        )

        for img_path in extracted_files:
            if not (img_path.suffix.lower() in ['.jpg', '.ppm', '.pbm']):
                continue

            # Calculate hash to skip duplicates
            with open(img_path, 'rb') as f:
                img_hash = hashlib.md5(f.read()).hexdigest()

            if img_hash in seen_hashes:
                print(f"Skipping duplicate: {img_path.name}")
                continue
            seen_hashes.add(img_hash)

            # Get image dimensions
            result = subprocess.run(
                ['magick', 'identify', '-format', '%w %h', str(img_path)],
                check=True, capture_output=True, text=True
            )
            width, height = map(int, result.stdout.split())

            # Split two-page spreads
            if width > height:
                print(f"Splitting two-page spread: {img_path.name}")
                half_width = width // 2
                
                page1_path = image_dir / f"page_{page_num:04d}.jpg"
                page2_path = image_dir / f"page_{page_num + 1:04d}.jpg"

                if left_to_right:
                    # Left page first
                    subprocess.run(
                        ['magick', str(img_path), '-crop', f'{half_width}x{height}+0+0', str(page1_path)],
                        check=True
                    )
                    # Right page second
                    subprocess.run(
                        ['magick', str(img_path), '-crop', f'{half_width}x{height}+{half_width}+0', str(page2_path)],
                        check=True
                    )
                else:
                    # Right page first
                    subprocess.run(
                        ['magick', str(img_path), '-crop', f'{half_width}x{height}+{half_width}+0', str(page1_path)],
                        check=True
                    )
                    # Left page second
                    subprocess.run(
                        ['magick', str(img_path), '-crop', f'{half_width}x{height}+0+0', str(page2_path)],
                        check=True
                    )
                
                with Image.open(page1_path) as img:
                    image_sizes.append(img.size)
                with Image.open(page2_path) as img:
                    image_sizes.append(img.size)
                
                shutil.move(page1_path, image_dir / f"page-{page_num}.jpg")
                shutil.move(page2_path, image_dir / f"page-{page_num + 1}.jpg")
                page_num += 2
            else:
                print(f"Single page: {img_path.name}")
                dest_path = image_dir / f"page-{page_num}.jpg"
                subprocess.run(
                    ['magick', str(img_path), str(dest_path)],
                    check=True
                )
                with Image.open(dest_path) as img:
                    image_sizes.append(img.size)
                page_num += 1
                
        return image_sizes

    finally:
        if temp_extract_dir.exists():
            shutil.rmtree(temp_extract_dir)


def create_nav_document(output_dir, title, num_pages):
    nav_content = f"""<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
	<head>
		<meta charset="utf-8" />
		<title>{title}</title>
	</head>
	<body>
		<nav epub:type="toc">
			<ol>
				<li><a href="page-1.xhtml">Cover</a></li>
			</ol>
		</nav>
	</body>
</html>
"""
    with open(output_dir / "OEBPS" / "nav.xhtml", 'w', encoding='utf-8') as f:
        f.write(nav_content)


def create_xhtml_pages(output_dir, num_pages, image_sizes):
    oebps_dir = output_dir / "OEBPS"

    for i in range(1, num_pages + 1):
        width, height = image_sizes[i - 1]
        xhtml_content = create_xhtml_page(i, f"page-{i}.jpg", width, height)

        with open(oebps_dir / f"page-{i}.xhtml", 'w', encoding='utf-8') as f:
            f.write(xhtml_content)


def create_epub_zip(temp_dir, output_epub):
    print(f"Packaging EPUB: {output_epub.name}")

    with zipfile.ZipFile(output_epub, 'w', zipfile.ZIP_DEFLATED) as epub:
        epub.write(temp_dir / "mimetype", "mimetype", compress_type=zipfile.ZIP_STORED)

        for root, dirs, files in os.walk(temp_dir):
            for file in files:
                if file == "mimetype":
                    continue
                file_path = Path(root) / file
                arcname = file_path.relative_to(temp_dir)
                epub.write(file_path, arcname, compress_type=zipfile.ZIP_DEFLATED)



def convert_pdf_to_epub(pdf_path, output_epub=None, left_to_right=False):
    conversion_start = time.time()
    pdf_path = Path(pdf_path)

    if not pdf_path.exists():
        print(f"Error: PDF file not found: {pdf_path}")
        return False

    if output_epub is None:
        output_epub = pdf_path.with_suffix('.epub')
    else:
        output_epub = Path(output_epub)

    title = pdf_path.stem + " (Manga)"
    pdf_size_mb = pdf_path.stat().st_size / (1024 * 1024)

    series_title = parse_manga_title(pdf_path.name)
    metadata = fetch_metadata(series_title)

    temp_dir = Path(tempfile.mkdtemp())

    try:
        print("Creating EPUB structure...")
        create_mimetype(temp_dir)
        create_container_xml(temp_dir)
        create_css(temp_dir)

        image_sizes = extract_and_process_images(pdf_path, temp_dir, left_to_right)
        num_pages = len(image_sizes)

        print("Generating XHTML pages...")
        create_xhtml_pages(temp_dir, num_pages, image_sizes)
        create_nav_document(temp_dir, title, num_pages)

        print("Writing metadata...")
        reading_direction = 'ltr' if left_to_right else 'rtl'
        create_content_opf(temp_dir, title, num_pages, image_sizes, metadata, reading_direction)

        create_epub_zip(temp_dir, output_epub)

        # Final statistics
        conversion_time = time.time() - conversion_start
        epub_size_mb = output_epub.stat().st_size / (1024 * 1024)
        compression_ratio = (1 - epub_size_mb / pdf_size_mb) * 100 if pdf_size_mb > 0 else 0

        print(f"\nConversion successful!")
        print(f"Total time: {conversion_time:.1f}s ({conversion_time/60:.1f} minutes)")
        print(f"PDF size: {pdf_size_mb:.1f}MB → EPUB size: {epub_size_mb:.1f}MB ({compression_ratio:+.0f}%)")
        print(f"Output: {output_epub}")

        return True

    except Exception as e:
        print(f"Error during conversion: {e}")
        import traceback
        traceback.print_exc()
        return False

    finally:
        if temp_dir.exists():
            shutil.rmtree(temp_dir)

    
def main():
    parser = argparse.ArgumentParser(
        description='Convert PDF files to EPUB format. By default, processes all PDFs in the current directory with right-to-left reading.'
    )
    parser.add_argument('pdf_files', nargs='*', help='Input PDF file(s) to convert. If not specified, converts all PDFs in current directory.')
    parser.add_argument('-o', '--output', help='Output EPUB file (only valid when converting a single file)')
    parser.add_argument('-ltr', '--left-to-right', action='store_true',
                       help='Use left-to-right reading direction (default is right-to-left for manga)')

    args = parser.parse_args()

    if args.output and len(args.pdf_files) > 1:
        print("Error: -o/--output can only be used when converting a single file")
        sys.exit(1)

    if args.pdf_files:
        pdf_paths = [Path(f) for f in args.pdf_files]
    else:
        pdf_paths = list(Path.cwd().glob('*.pdf'))
        pdf_paths.extend(Path.cwd().glob('*.PDF'))

    if not pdf_paths:
        print("No PDF files found")
        sys.exit(1)

    epub_output_dir = Path.cwd()
    print(f"Output directory: {epub_output_dir}")

    print(f"Converting {len(pdf_paths)} PDF file(s)")
    print(f"Reading direction: {'Left-to-Right' if args.left_to_right else 'Right-to-Left'}")

    success_count = 0
    for pdf_path in pdf_paths:
        print(f"\n{'='*60}")
        print(f"Processing: {pdf_path.name}")
        print(f"{'='*60}")

        if args.output:
            output_epub = Path(args.output)
        else:
            # Place output in EPUB directory
            epub_filename = pdf_path.stem + " (Manga).epub"
            output_epub = epub_output_dir / epub_filename

        success = convert_pdf_to_epub(pdf_path, output_epub, args.left_to_right)

        if success:
            success_count += 1

    print(f"\n{'='*60}")
    print(f"Conversion complete: {success_count}/{len(pdf_paths)} successful")
    print(f"{'='*60}")

    sys.exit(0 if success_count == len(pdf_paths) else 1)


if __name__ == '__main__':
    main()
