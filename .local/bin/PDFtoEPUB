#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.9"
# dependencies = [
#     "pillow",
# ]
# ///

import sys
import argparse
import uuid
import subprocess
from pathlib import Path
from datetime import datetime
import zipfile
import shutil
import os
from PIL import Image
import re
import json
import tempfile
try:
    import urllib.request
    import urllib.parse
except ImportError:
    pass



def parse_manga_title(filename):
    name = filename.replace('.pdf', '').replace('.PDF', '')

    patterns = [
        r'\s*[-_]\s*v(?:ol(?:ume)?\.?)?\s*\d+.*$',
        r'\s*v(?:ol(?:ume)?\.?)?\s*\d+.*$',
        r'\s*[-_]\s*\d+.*$',
        r'\s*#\d+.*$',
        r'\s*\(\d+\).*$',
        r'\s*\[\d+\].*$',
    ]

    for pattern in patterns:
        name = re.sub(pattern, '', name, flags=re.IGNORECASE)

    name = re.sub(r'\s+', ' ', name).strip()

    return name


def fetch_anilist_metadata(title):
    try:
        query = '''
        query ($search: String) {
          Media(search: $search, type: MANGA) {
            staff {
              edges {
                role
                node {
                  name {
                    full
                  }
                }
              }
            }
            startDate {
              year
              month
              day
            }
            endDate {
              year
              month
              day
            }
            genres
          }
        }
        '''

        variables = {'search': title}

        data = json.dumps({
            'query': query,
            'variables': variables
        }).encode('utf-8')

        req = urllib.request.Request(
            'https://graphql.anilist.co',
            data=data,
            headers={'Content-Type': 'application/json'}
        )

        with urllib.request.urlopen(req, timeout=5) as response:
            result = json.loads(response.read().decode('utf-8'))

            if 'data' in result and result['data']['Media']:
                media = result['data']['Media']

                author = None
                if 'staff' in media and media['staff']['edges']:
                    for edge in media['staff']['edges']:
                        role = edge['role']
                        if any(keyword in role for keyword in ['Story & Art', 'Story', 'Original Creator']):
                            author = edge['node']['name']['full']
                            break

                # Format dates
                start_date = None
                if media.get('startDate') and media['startDate'].get('year'):
                    start_date = f"{media['startDate']['year']}"

                return {
                    'author': author,
                    'publication_date': start_date,
                    'genres': media.get('genres', [])
                }

        return None
    except Exception as e:
        print(f"Could not fetch AniList metadata: {e}")
        return None


def create_mimetype(output_dir):
    mimetype_path = output_dir / "mimetype"
    with open(mimetype_path, 'w', encoding='utf-8') as f:
        f.write("application/epub+zip")
    return mimetype_path


def create_container_xml(output_dir):
    meta_inf = output_dir / "META-INF"
    meta_inf.mkdir(exist_ok=True)

    container_xml = """<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<container version="1.0" xmlns="urn:oasis:names:tc:opendocument:xmlns:container">
\t<rootfiles>
\t\t<rootfile full-path="OEBPS/content.opf" media-type="application/oebps-package+xml" />
\t</rootfiles>
</container>
"""

    with open(meta_inf / "container.xml", 'w', encoding='utf-8') as f:
        f.write(container_xml)


def create_css(output_dir):
    css_dir = output_dir / "OEBPS" / "css"
    css_dir.mkdir(parents=True, exist_ok=True)

    css_content = """body, div, dl, dt, dd, h1, h2, h3, h4, h5, h6, p, pre, code, blockquote {
\tmargin:0;
\tpadding:0;
\tborder-width:0;
\ttext-rendering:optimizeSpeed;
}
body {
\tmargin:0;
\tpadding:0;
}
.page-container {
\tposition:absolute;
\tleft:0px;
\ttop:0px;
\twidth:100%;
\theight:100%;
}
img.full-page {
\theight:100%;
\twidth:100%;
\tobject-fit:contain;
\tposition:absolute;
\tleft:0px;
\ttop:0px;
}
"""

    with open(css_dir / "style.css", 'w', encoding='utf-8') as f:
        f.write(css_content)


def create_xhtml_page(page_num, image_filename, image_width, image_height):
    xhtml = f"""<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
\t<head>
\t\t<meta charset="utf-8" />
\t\t<meta name="viewport" content="width={image_width},height={image_height}" />
\t\t<title>Page {page_num}</title>
\t\t<link href="css/style.css" rel="stylesheet" type="text/css" />
\t</head>
\t<body style="width:{image_width}px;height:{image_height}px;margin:0;padding:0">
\t\t<div class="page-container">
\t\t\t<img class="full-page" src="image/{image_filename}" alt="Page {page_num}" />
\t\t</div>
\t</body>
</html>
"""
    return xhtml


def create_content_opf(output_dir, title, num_pages, image_sizes, metadata=None, reading_direction='rtl'):
    book_id = str(uuid.uuid4())
    timestamp = datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ")

    author = metadata.get('author') if metadata else None
    pub_date = metadata.get('publication_date') if metadata else None
    genres = metadata.get('genres', []) if metadata else []

    manifest_pages = []
    for i in range(1, num_pages + 1):
        manifest_pages.append(f'\t\t<item id="page-{i}" href="page-{i}.xhtml" media-type="application/xhtml+xml" />')

    manifest_images = []
    for i in range(1, num_pages + 1):
        properties = ' properties="cover-image"' if i == 1 else ''
        manifest_images.append(f'\t\t<item id="img-{i}" href="image/page-{i}.png" media-type="image/png"{properties} />')

    spine_items = []
    for i in range(1, num_pages + 1):
        spine_items.append(f'\t\t<itemref idref="page-{i}" />')

    metadata_lines = [
        f'\t\t<meta name="cover" content="img-1" />',
        f'\t\t<dc:title>{title}</dc:title>',
    ]

    if author:
        metadata_lines.append(f'\t\t<dc:creator>{author}</dc:creator>')

    if pub_date:
        metadata_lines.append(f'\t\t<dc:date>{pub_date}</dc:date>')
    else:
        metadata_lines.append(f'\t\t<dc:date>{timestamp}</dc:date>')

    for genre in genres:
        metadata_lines.append(f'\t\t<dc:subject>{genre}</dc:subject>')

    metadata_lines.extend([
        f'\t\t<dc:language>en</dc:language>',
        f'\t\t<meta property="dcterms:modified">{timestamp}</meta>',
        f'\t\t<dc:identifier id="bookid">urn:uuid:{book_id}</dc:identifier>',
        f'\t\t<!--fixed-layout options-->',
        f'\t\t<meta property="rendition:layout">pre-paginated</meta>',
        f'\t\t<meta property="rendition:spread">auto</meta>',
    ])

    content_opf = f"""<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<package version="3.0" xmlns="http://www.idpf.org/2007/opf" unique-identifier="bookid" prefix="rendition: http://www.idpf.org/vocab/rendition/# ibooks: http://vocabulary.itunes.apple.com/rdf/ibooks/vocabulary-extensions-1.0/">
\t<metadata xmlns:dc="http://purl.org/dc/elements/1.1/">
{chr(10).join(metadata_lines)}
\t</metadata>
\t<manifest>
{chr(10).join(manifest_pages)}
\t\t<item id="nav" href="nav.xhtml" media-type="application/xhtml+xml" properties="nav" />
\t\t<item id="css" href="css/style.css" media-type="text/css" />
{chr(10).join(manifest_images)}
\t</manifest>
\t<spine page-progression-direction="{reading_direction}">
{chr(10).join(spine_items)}
\t</spine>
</package>
"""

    with open(output_dir / "OEBPS" / "content.opf", 'w', encoding='utf-8') as f:
        f.write(content_opf)


def disable_hidden_layers(pdf_path, temp_dir):
    """
    Processes the PDF to remove or flatten hidden layers if possible.
    Returns the path to the processed PDF.
    """
    output_pdf_path = temp_dir / pdf_path.name
    
    # Check if qpdf is available
    if not shutil.which("qpdf"):
        print("Warning: qpdf not found. Cannot remove hidden layers. Proceeding with original PDF.")
        shutil.copy(pdf_path, output_pdf_path)
        return output_pdf_path

    print(f"Attempting to remove hidden layers using qpdf: {pdf_path.name}")
    try:
        # qpdf command to flatten annotations and remove unnecessary content.
        # This might help with some forms of "hidden" content, but true OCG layers
        # are more complex. For a general approach, flattening is often the best bet.
        qpdf_result = subprocess.run(
            ['qpdf', str(pdf_path), '--flatten-annotations=all', str(output_pdf_path)],
            capture_output=True,
            text=True
        )

        if qpdf_result.returncode == 0:
            print(f"Processed PDF saved to: {output_pdf_path}")
            return output_pdf_path
        elif qpdf_result.returncode != 0 and "operation succeeded with warnings" in qpdf_result.stderr:

            # Suppress warning output as requested
            return output_pdf_path
        else:
            print(f"Error processing PDF with qpdf (exit code {qpdf_result.returncode}): {qpdf_result.stderr.strip()}")
            print("Proceeding with original PDF without removing hidden layers.")
            shutil.copy(pdf_path, output_pdf_path)
    
    except Exception as e:
        print(f"An unexpected error occurred with qpdf: {e}")
        print("Proceeding with original PDF without removing hidden layers.")
        shutil.copy(pdf_path, output_pdf_path)
        return output_pdf_path


def convert_pdf_to_images(pdf_path, output_dir, dpi):
    print(f"Rendering PDF pages to images")

    image_dir = output_dir / "OEBPS" / "image"
    image_dir.mkdir(parents=True, exist_ok=True)

    temp_extract = output_dir / "temp_images"
    temp_extract.mkdir(exist_ok=True)
    
    layer_temp_dir = Path(tempfile.mkdtemp(prefix="layer_fix_"))
    processed_pdf_path = None # To store path of PDF with layers possibly removed

    try:
        # Disable hidden layers and get path to the processed PDF
        processed_pdf_path = disable_hidden_layers(pdf_path, layer_temp_dir)

        # Get number of pages from the processed PDF
        pdfinfo_result = subprocess.run(
            ['pdfinfo', str(processed_pdf_path)],
            check=True, capture_output=True, text=True
        )
        num_pages = int(re.search(r'Pages:\s*(\d+)', pdfinfo_result.stdout).group(1))
        print(f"Detected {num_pages} pages")

        print(f"Rendering pages as PNGs at {dpi} DPI using mutool...")
        # Use mutool to render pages, as it handles layers correctly and was requested by the user.
        subprocess.run([
            'mutool',
            'draw',
            '-o', str(temp_extract / 'page-%d.png'),
            '-r', str(dpi),
            str(processed_pdf_path)
        ], check=True, capture_output=True)

        print(f"Rendering complete, processing images...")

        image_sizes = []
        # pdftoppm names pages starting from 1, with leading zeros
        # e.g., page-01.png, page-02.png ... page-10.png
        # We need to sort them numerically
        image_files = sorted(
            [f for f in temp_extract.iterdir() if f.is_file() and f.suffix == '.png'],
            key=lambda f: int(f.stem.split('-')[-1])
        )

        if not image_files:
            raise RuntimeError("No images rendered from PDF")

        for i, image_path in enumerate(image_files, start=1):
            with Image.open(image_path) as img:
                width, height = img.size
                image_sizes.append((width, height))

                final_path = image_dir / f"page-{i}.png"
                # Just move the file, no need to re-save
                shutil.move(image_path, final_path)
                print(f"  Processing page {i}/{num_pages}: {width}x{height}")

        return image_sizes

    finally:
        if temp_extract.exists():
            shutil.rmtree(temp_extract)
        if layer_temp_dir.exists():
            shutil.rmtree(layer_temp_dir)


def create_nav_document(output_dir, title, num_pages):
    nav_content = f"""<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
\t<head>
\t\t<meta charset="utf-8" />
\t\t<title>{title}</title>
\t</head>
\t<body>
\t\t<nav epub:type="toc">
\t\t\t<ol>
\t\t\t\t<li><a href="page-1.xhtml">Cover</a></li>
\t\t\t</ol>
\t\t</nav>
\t</body>
</html>
"""
    with open(output_dir / "OEBPS" / "nav.xhtml", 'w', encoding='utf-8') as f:
        f.write(nav_content)


def create_xhtml_pages(output_dir, num_pages, image_sizes):
    oebps_dir = output_dir / "OEBPS"

    for i in range(1, num_pages + 1):
        width, height = image_sizes[i - 1]
        xhtml_content = create_xhtml_page(i, f"page-{i}.png", width, height)

        with open(oebps_dir / f"page-{i}.xhtml", 'w', encoding='utf-8') as f:
            f.write(xhtml_content)


def create_epub_zip(temp_dir, output_epub):
    print(f"Creating EPUB: {output_epub}")

    with zipfile.ZipFile(output_epub, 'w', zipfile.ZIP_DEFLATED) as epub:
        epub.write(temp_dir / "mimetype", "mimetype", compress_type=zipfile.ZIP_STORED)

        # Add all other files
        for root, dirs, files in os.walk(temp_dir):
            for file in files:
                if file == "mimetype":
                    continue
                file_path = Path(root) / file
                arcname = file_path.relative_to(temp_dir)
                epub.write(file_path, arcname, compress_type=zipfile.ZIP_DEFLATED)




def convert_pdf_to_epub(pdf_path, output_epub=None, left_to_right=False, dpi=600):
    pdf_path = Path(pdf_path)

    if not pdf_path.exists():
        print(f"Error: PDF file not found: {pdf_path}")
        return False

    if output_epub is None:
        output_epub = pdf_path.with_suffix('.epub')
    else:
        output_epub = Path(output_epub)

    title = pdf_path.stem

    print("Fetching metadata...")
    series_title = parse_manga_title(pdf_path.name)
    print(f"  Parsed series title: '{series_title}'")

    metadata = fetch_anilist_metadata(series_title)
    if metadata:
        if metadata.get('author'):
            print(f"  Found author: {metadata['author']}")
        if metadata.get('publication_date'):
            print(f"  Publication date: {metadata['publication_date']}")
        if metadata.get('genres'):
            print(f"  Genres: {', '.join(metadata['genres'])}")
    else:
        print("  No metadata found online")

    temp_dir = Path(tempfile.mkdtemp())

    try:
        print("Creating EPUB structure...")
        create_mimetype(temp_dir)
        create_container_xml(temp_dir)
        create_css(temp_dir)

        image_sizes = convert_pdf_to_images(pdf_path, temp_dir, dpi)



        num_pages = len(image_sizes)

        print("Creating XHTML pages...")
        create_xhtml_pages(temp_dir, num_pages, image_sizes)
        create_nav_document(temp_dir, title, num_pages)

        print("Creating metadata...")
        reading_direction = 'ltr' if left_to_right else 'rtl'
        create_content_opf(temp_dir, title, num_pages, image_sizes, metadata, reading_direction)

        create_epub_zip(temp_dir, output_epub)

        return True

    except Exception as e:
        print(f"Error during conversion: {e}")
        import traceback
        traceback.print_exc()
        return False

    finally:
        if temp_dir.exists():
            print(f"Cleaning up temporary directory: {temp_dir}")
            shutil.rmtree(temp_dir)

    
def main():
    parser = argparse.ArgumentParser(
        description='Convert PDF files to EPUB format. By default, processes all PDFs in the current directory with right-to-left reading.'
    )
    parser.add_argument('pdf_files', nargs='*', help='Input PDF file(s) to convert. If not specified, converts all PDFs in current directory.')
    parser.add_argument('-o', '--output', help='Output EPUB file (only valid when converting a single file)')
    parser.add_argument('-ltr', '--left-to-right', action='store_true',
                       help='Use left-to-right reading direction (default is right-to-left for manga)')
    parser.add_argument('-d', '--dpi', type=int, default=600,
                       help='DPI (dots per inch) for rendering PDF pages to images (default: 600)')

    args = parser.parse_args()

    if args.output and len(args.pdf_files) > 1:
        print("Error: -o/--output can only be used when converting a single file")
        sys.exit(1)

    if args.pdf_files:
        pdf_paths = [Path(f) for f in args.pdf_files]
    else:
        pdf_paths = list(Path.cwd().glob('*.pdf'))
        pdf_paths.extend(Path.cwd().glob('*.PDF'))

    if not pdf_paths:
        print("No PDF files found.")
        sys.exit(1)

    # Create EPUB output directory
    epub_output_dir = Path.cwd() / "EPUB"
    epub_output_dir.mkdir(exist_ok=True)
    print(f"Output directory: {epub_output_dir}")

    print(f"Found {len(pdf_paths)} PDF file(s) to convert")
    print(f"Reading direction: {'Left-to-Right' if args.left_to_right else 'Right-to-Left'}")

    success_count = 0
    for pdf_path in pdf_paths:
        print(f"\n{'='*60}")
        print(f"Processing: {pdf_path.name}")
        print(f"{'='*60}")

        if args.output:
            output_epub = Path(args.output)
        else:
            # Place output in EPUB directory
            output_epub = epub_output_dir / pdf_path.with_suffix('.epub').name

        success = convert_pdf_to_epub(pdf_path, output_epub, args.left_to_right, args.dpi)

        if success:
            success_count += 1

    print(f"\n{'='*60}")
    print(f"Conversion complete: {success_count}/{len(pdf_paths)} successful")
    print(f"{'='*60}")

    sys.exit(0 if success_count == len(pdf_paths) else 1)


if __name__ == '__main__':
    main()
