#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.9"
# dependencies = [
#     "pillow",
# ]
# ///

import sys
import argparse
import uuid
import subprocess
from pathlib import Path
from datetime import datetime
import zipfile
import shutil
import os
from PIL import Image
import re
import json
import tempfile
import time
from multiprocessing import Pool, cpu_count
try:
    import urllib.request
    import urllib.parse
except ImportError:
    pass



def parse_manga_title(filename):
    name = filename.replace('.pdf', '').replace('.PDF', '')

    patterns = [
        r'\s*[-_]\s*v(?:ol(?:ume)?\.?)?\s*\d+.*$',
        r'\s*v(?:ol(?:ume)?\.?)?\s*\d+.*$',
        r'\s*[-_]\s*\d+.*$',
        r'\s*#\d+.*$',
        r'\s*\(\d+\).*$',
        r'\s*\[\d+\].*$',
    ]

    for pattern in patterns:
        name = re.sub(pattern, '', name, flags=re.IGNORECASE)

    name = re.sub(r'\s+', ' ', name).strip()

    return name


def fetch_anilist_metadata(title):
    """Fetch metadata from AniList (manga/anime database)"""
    try:
        query = '''
        query ($search: String) {
          Media(search: $search, type: MANGA) {
            staff {
              edges {
                role
                node {
                  name {
                    full
                  }
                }
              }
            }
            startDate {
              year
              month
              day
            }
            endDate {
              year
              month
              day
            }
          }
        }
        '''

        variables = {'search': title}

        data = json.dumps({
            'query': query,
            'variables': variables
        }).encode('utf-8')

        req = urllib.request.Request(
            'https://graphql.anilist.co',
            data=data,
            headers={'Content-Type': 'application/json'}
        )

        with urllib.request.urlopen(req, timeout=5) as response:
            result = json.loads(response.read().decode('utf-8'))

            if 'data' in result and result['data']['Media']:
                media = result['data']['Media']

                author = None
                if 'staff' in media and media['staff']['edges']:
                    for edge in media['staff']['edges']:
                        role = edge['role']
                        if any(keyword in role for keyword in ['Story & Art', 'Story', 'Original Creator']):
                            author = edge['node']['name']['full']
                            break

                # Format dates
                start_date = None
                if media.get('startDate') and media['startDate'].get('year'):
                    start_date = f"{media['startDate']['year']}"

                return {
                    'author': author,
                    'publication_date': start_date,
                    'source': 'AniList'
                }

        return None
    except Exception as e:
        print(f"  AniList: {e}")
        return None


def fetch_google_books_metadata(title):
    """Fetch metadata from Google Books API"""
    try:
        query = urllib.parse.quote(title)
        url = f"https://www.googleapis.com/books/v1/volumes?q={query}&maxResults=1"

        req = urllib.request.Request(url)
        with urllib.request.urlopen(req, timeout=5) as response:
            result = json.loads(response.read().decode('utf-8'))

            if result.get('totalItems', 0) > 0:
                book = result['items'][0]['volumeInfo']

                author = None
                if book.get('authors'):
                    author = ', '.join(book['authors'])

                pub_date = None
                if book.get('publishedDate'):
                    # Extract just the year
                    pub_date = book['publishedDate'][:4]

                return {
                    'author': author,
                    'publication_date': pub_date,
                    'source': 'Google Books'
                }

        return None
    except Exception as e:
        print(f"  Google Books: {e}")
        return None


def fetch_openlibrary_metadata(title):
    """Fetch metadata from Open Library API"""
    try:
        query = urllib.parse.quote(title)
        url = f"https://openlibrary.org/search.json?title={query}&limit=1"

        req = urllib.request.Request(url)
        with urllib.request.urlopen(req, timeout=5) as response:
            result = json.loads(response.read().decode('utf-8'))

            if result.get('numFound', 0) > 0:
                book = result['docs'][0]

                author = None
                if book.get('author_name'):
                    author = ', '.join(book['author_name'][:2])  # Limit to first 2 authors

                pub_date = None
                if book.get('first_publish_year'):
                    pub_date = str(book['first_publish_year'])

                return {
                    'author': author,
                    'publication_date': pub_date,
                    'source': 'Open Library'
                }

        return None
    except Exception as e:
        print(f"  Open Library: {e}")
        return None


def fetch_mangadex_metadata(title):
    """Fetch metadata from MangaDex API"""
    try:
        query = urllib.parse.quote(title)
        url = f"https://api.mangadex.org/manga?title={query}&limit=1&includes[]=author&includes[]=artist"

        req = urllib.request.Request(url)
        with urllib.request.urlopen(req, timeout=5) as response:
            result = json.loads(response.read().decode('utf-8'))

            if result.get('data') and len(result['data']) > 0:
                manga = result['data'][0]

                # Extract author from relationships
                author = None
                for rel in manga.get('relationships', []):
                    if rel['type'] in ['author', 'artist']:
                        author_name = rel.get('attributes', {}).get('name')
                        if author_name:
                            author = author_name
                            break

                # Get publication year
                pub_date = None
                attrs = manga.get('attributes', {})
                if attrs.get('year'):
                    pub_date = str(attrs['year'])

                return {
                    'author': author,
                    'publication_date': pub_date,
                    'source': 'MangaDex'
                }

        return None
    except Exception as e:
        print(f"  MangaDex: {e}")
        return None


def fetch_metadata(title):
    """
    Fetch metadata from multiple sources with fallback.
    Tries sources in order until finding good metadata.
    """
    print(f"Searching for metadata: '{title}'")

    # Try each source in order of preference
    sources = [
        fetch_anilist_metadata,      # Best for manga/anime
        fetch_mangadex_metadata,      # Manga-specific
        fetch_google_books_metadata,  # General books
        fetch_openlibrary_metadata,   # Comprehensive fallback
    ]

    for fetch_func in sources:
        metadata = fetch_func(title)
        if metadata and metadata.get('author'):  # Only accept if we found an author
            source = metadata.get('source', 'Unknown')
            print(f"  ✓ Found metadata from {source}")
            if metadata.get('author'):
                print(f"    Author: {metadata['author']}")
            if metadata.get('publication_date'):
                print(f"    Published: {metadata['publication_date']}")
            return metadata

    print(f"  ✗ No metadata found from any source")
    return None


def create_mimetype(output_dir):
    mimetype_path = output_dir / "mimetype"
    with open(mimetype_path, 'w', encoding='utf-8') as f:
        f.write("application/epub+zip")
    return mimetype_path


def create_container_xml(output_dir):
    meta_inf = output_dir / "META-INF"
    meta_inf.mkdir(exist_ok=True)

    container_xml = """<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<container version="1.0" xmlns="urn:oasis:names:tc:opendocument:xmlns:container">
\t<rootfiles>
\t\t<rootfile full-path="OEBPS/content.opf" media-type="application/oebps-package+xml" />
\t</rootfiles>
</container>
"""

    with open(meta_inf / "container.xml", 'w', encoding='utf-8') as f:
        f.write(container_xml)


def create_css(output_dir):
    css_dir = output_dir / "OEBPS" / "css"
    css_dir.mkdir(parents=True, exist_ok=True)

    css_content = """body, div, dl, dt, dd, h1, h2, h3, h4, h5, h6, p, pre, code, blockquote {
\tmargin:0;
\tpadding:0;
\tborder-width:0;
\ttext-rendering:optimizeSpeed;
}
body {
\tmargin:0;
\tpadding:0;
}
.page-container {
\tposition:absolute;
\tleft:0px;
\ttop:0px;
\twidth:100%;
\theight:100%;
}
img.full-page {
\theight:100%;
\twidth:100%;
\tobject-fit:contain;
\tposition:absolute;
\tleft:0px;
\ttop:0px;
}
"""

    with open(css_dir / "style.css", 'w', encoding='utf-8') as f:
        f.write(css_content)


def create_xhtml_page(page_num, image_filename, image_width, image_height):
    xhtml = f"""<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
\t<head>
\t\t<meta charset="utf-8" />
\t\t<meta name="viewport" content="width={image_width},height={image_height}" />
\t\t<title>Page {page_num}</title>
\t\t<link href="css/style.css" rel="stylesheet" type="text/css" />
\t</head>
\t<body style="width:{image_width}px;height:{image_height}px;margin:0;padding:0">
\t\t<div class="page-container">
\t\t\t<img class="full-page" src="image/{image_filename}" alt="Page {page_num}" />
\t\t</div>
\t</body>
</html>
"""
    return xhtml


def create_content_opf(output_dir, title, num_pages, image_sizes, metadata=None, reading_direction='rtl'):
    book_id = str(uuid.uuid4())
    timestamp = datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ")

    author = metadata.get('author') if metadata else None
    pub_date = metadata.get('publication_date') if metadata else None

    manifest_pages = []
    for i in range(1, num_pages + 1):
        manifest_pages.append(f'\t\t<item id="page-{i}" href="page-{i}.xhtml" media-type="application/xhtml+xml" />')

    manifest_images = []
    for i in range(1, num_pages + 1):
        properties = ' properties="cover-image"' if i == 1 else ''
        manifest_images.append(f'\t\t<item id="img-{i}" href="image/page-{i}.jpg" media-type="image/jpeg"{properties} />')

    spine_items = []
    for i in range(1, num_pages + 1):
        spine_items.append(f'\t\t<itemref idref="page-{i}" />')

    metadata_lines = [
        f'\t\t<meta name="cover" content="img-1" />',
        f'\t\t<dc:title>{title}</dc:title>',
    ]

    if author:
        metadata_lines.append(f'\t\t<dc:creator>{author}</dc:creator>')

    if pub_date:
        metadata_lines.append(f'\t\t<dc:date>{pub_date}</dc:date>')
    else:
        metadata_lines.append(f'\t\t<dc:date>{timestamp}</dc:date>')

    metadata_lines.extend([
        f'\t\t<dc:language>en</dc:language>',
        f'\t\t<meta property="dcterms:modified">{timestamp}</meta>',
        f'\t\t<dc:identifier id="bookid">urn:uuid:{book_id}</dc:identifier>',
        f'\t\t<meta property="rendition:layout">pre-paginated</meta>',
        f'\t\t<meta property="rendition:spread">auto</meta>',
    ])

    content_opf = f"""<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<package version="3.0" xmlns="http://www.idpf.org/2007/opf" unique-identifier="bookid" prefix="rendition: http://www.idpf.org/vocab/rendition/# ibooks: http://vocabulary.itunes.apple.com/rdf/ibooks/vocabulary-extensions-1.0/">
\t<metadata xmlns:dc="http://purl.org/dc/elements/1.1/">
{chr(10).join(metadata_lines)}
\t</metadata>
\t<manifest>
{chr(10).join(manifest_pages)}
\t\t<item id="nav" href="nav.xhtml" media-type="application/xhtml+xml" properties="nav" />
\t\t<item id="css" href="css/style.css" media-type="text/css" />
{chr(10).join(manifest_images)}
\t</manifest>
\t<spine page-progression-direction="{reading_direction}">
{chr(10).join(spine_items)}
\t</spine>
</package>
"""

    with open(output_dir / "OEBPS" / "content.opf", 'w', encoding='utf-8') as f:
        f.write(content_opf)


def disable_hidden_layers(pdf_path, temp_dir):
    """
    Removes hidden layers (OCG) from PDF using mutool clean.
    Returns the path to the processed PDF.
    """
    output_pdf_path = temp_dir / pdf_path.name

    if not shutil.which("mutool"):
        print("Warning: mutool not found, skipping layer removal")
        shutil.copy(pdf_path, output_pdf_path)
        return output_pdf_path

    print(f"Removing hidden layers from PDF...")
    try:
        # mutool clean -gggg removes OCG layers, annotations, and unused objects
        mutool_result = subprocess.run(
            ['mutool', 'clean', '-gggg', str(pdf_path), str(output_pdf_path)],
            capture_output=True,
            text=True
        )

        if mutool_result.returncode == 0:
            print(f"Layer removal complete")
            return output_pdf_path
        else:
            print(f"Layer removal failed: {mutool_result.stderr.strip()}")
            print("Proceeding with original PDF")
            shutil.copy(pdf_path, output_pdf_path)

    except Exception as e:
        print(f"Error: {e}")
        print("Proceeding with original PDF")
        shutil.copy(pdf_path, output_pdf_path)
        return output_pdf_path


def process_single_image(args):
    """Helper function to process a single image in parallel"""
    image_path, page_num, image_dir, jpeg_quality = args
    try:
        with Image.open(image_path) as img:
            width, height = img.size

            # Convert to RGB (in case of RGBA) and save as optimized JPEG
            final_path = image_dir / f"page-{page_num}.jpg"
            rgb_img = img.convert('RGB')
            rgb_img.save(
                final_path,
                'JPEG',
                quality=jpeg_quality,
                optimize=True,
                progressive=True
            )

            # Get final file size for logging
            final_size = final_path.stat().st_size / 1024  # KB
            return (page_num, width, height, final_size)
    except Exception as e:
        print(f"Error processing page {page_num}: {e}")
        return None


def convert_pdf_to_images(pdf_path, output_dir, dpi, jpeg_quality=80):
    print(f"Converting PDF to images at {dpi} DPI...")

    image_dir = output_dir / "OEBPS" / "image"
    image_dir.mkdir(parents=True, exist_ok=True)

    temp_extract = output_dir / "temp_images"
    temp_extract.mkdir(exist_ok=True)

    layer_temp_dir = Path(tempfile.mkdtemp(prefix="layer_fix_"))
    processed_pdf_path = None

    try:
        processed_pdf_path = disable_hidden_layers(pdf_path, layer_temp_dir)

        # Get page count
        pdfinfo_result = subprocess.run(
            ['pdfinfo', str(processed_pdf_path)],
            check=True, capture_output=True, text=True
        )
        num_pages = int(re.search(r'Pages:\s*(\d+)', pdfinfo_result.stdout).group(1))
        print(f"Pages: {num_pages}")

        print(f"Rendering {num_pages} pages to PNG at {dpi} DPI (this may take a while)...")
        render_start = time.time()
        subprocess.run([
            'mutool',
            'draw',
            '-o', str(temp_extract / 'page-%d.png'),
            '-r', str(dpi),
            str(processed_pdf_path)
        ], check=True)
        render_time = time.time() - render_start
        print(f"Rendering complete ({render_time:.1f}s, {render_time/num_pages:.1f}s per page)")

        print(f"Converting to JPEG (quality {jpeg_quality})...")
        convert_start = time.time()

        image_files = sorted(
            [f for f in temp_extract.iterdir() if f.is_file() and f.suffix == '.png'],
            key=lambda f: int(f.stem.split('-')[-1])
        )

        if not image_files:
            raise RuntimeError("No images rendered from PDF")

        process_args = [
            (image_path, i, image_dir, jpeg_quality)
            for i, image_path in enumerate(image_files, start=1)
        ]

        num_workers = min(cpu_count(), len(image_files))
        print(f"Using {num_workers} parallel workers...")

        image_sizes = []
        with Pool(num_workers) as pool:
            results = pool.map(process_single_image, process_args)

            results = [r for r in results if r is not None]
            results.sort(key=lambda x: x[0])

            for page_num, width, height, final_size in results:
                image_sizes.append((width, height))
                print(f"Page {page_num}/{num_pages}: {width}x{height} ({final_size:.0f}KB)")

        convert_time = time.time() - convert_start
        print(f"Conversion complete ({convert_time:.1f}s)")

        return image_sizes

    finally:
        if temp_extract.exists():
            shutil.rmtree(temp_extract)
        if layer_temp_dir.exists():
            shutil.rmtree(layer_temp_dir)


def create_nav_document(output_dir, title, num_pages):
    nav_content = f"""<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
\t<head>
\t\t<meta charset="utf-8" />
\t\t<title>{title}</title>
\t</head>
\t<body>
\t\t<nav epub:type="toc">
\t\t\t<ol>
\t\t\t\t<li><a href="page-1.xhtml">Cover</a></li>
\t\t\t</ol>
\t\t</nav>
\t</body>
</html>
"""
    with open(output_dir / "OEBPS" / "nav.xhtml", 'w', encoding='utf-8') as f:
        f.write(nav_content)


def create_xhtml_pages(output_dir, num_pages, image_sizes):
    oebps_dir = output_dir / "OEBPS"

    for i in range(1, num_pages + 1):
        width, height = image_sizes[i - 1]
        xhtml_content = create_xhtml_page(i, f"page-{i}.jpg", width, height)

        with open(oebps_dir / f"page-{i}.xhtml", 'w', encoding='utf-8') as f:
            f.write(xhtml_content)


def create_epub_zip(temp_dir, output_epub):
    print(f"Packaging EPUB: {output_epub.name}")

    with zipfile.ZipFile(output_epub, 'w', zipfile.ZIP_DEFLATED) as epub:
        epub.write(temp_dir / "mimetype", "mimetype", compress_type=zipfile.ZIP_STORED)

        for root, dirs, files in os.walk(temp_dir):
            for file in files:
                if file == "mimetype":
                    continue
                file_path = Path(root) / file
                arcname = file_path.relative_to(temp_dir)
                epub.write(file_path, arcname, compress_type=zipfile.ZIP_DEFLATED)




def convert_pdf_to_epub(pdf_path, output_epub=None, left_to_right=False, dpi=150, jpeg_quality=80):
    conversion_start = time.time()
    pdf_path = Path(pdf_path)

    if not pdf_path.exists():
        print(f"Error: PDF file not found: {pdf_path}")
        return False

    if output_epub is None:
        output_epub = pdf_path.with_suffix('.epub')
    else:
        output_epub = Path(output_epub)

    title = pdf_path.stem + " (Manga)"
    pdf_size_mb = pdf_path.stat().st_size / (1024 * 1024)

    series_title = parse_manga_title(pdf_path.name)
    metadata = fetch_metadata(series_title)

    temp_dir = Path(tempfile.mkdtemp())

    try:
        print("Creating EPUB structure...")
        create_mimetype(temp_dir)
        create_container_xml(temp_dir)
        create_css(temp_dir)

        image_sizes = convert_pdf_to_images(pdf_path, temp_dir, dpi, jpeg_quality)
        num_pages = len(image_sizes)

        print("Generating XHTML pages...")
        create_xhtml_pages(temp_dir, num_pages, image_sizes)
        create_nav_document(temp_dir, title, num_pages)

        print("Writing metadata...")
        reading_direction = 'ltr' if left_to_right else 'rtl'
        create_content_opf(temp_dir, title, num_pages, image_sizes, metadata, reading_direction)

        create_epub_zip(temp_dir, output_epub)

        # Final statistics
        conversion_time = time.time() - conversion_start
        epub_size_mb = output_epub.stat().st_size / (1024 * 1024)
        compression_ratio = (1 - epub_size_mb / pdf_size_mb) * 100 if pdf_size_mb > 0 else 0

        print(f"\nConversion successful!")
        print(f"Total time: {conversion_time:.1f}s ({conversion_time/60:.1f} minutes)")
        print(f"PDF size: {pdf_size_mb:.1f}MB → EPUB size: {epub_size_mb:.1f}MB ({compression_ratio:+.0f}%)")
        print(f"Output: {output_epub}")

        return True

    except Exception as e:
        print(f"Error during conversion: {e}")
        import traceback
        traceback.print_exc()
        return False

    finally:
        if temp_dir.exists():
            shutil.rmtree(temp_dir)

    
def main():
    parser = argparse.ArgumentParser(
        description='Convert PDF files to EPUB format. By default, processes all PDFs in the current directory with right-to-left reading.'
    )
    parser.add_argument('pdf_files', nargs='*', help='Input PDF file(s) to convert. If not specified, converts all PDFs in current directory.')
    parser.add_argument('-o', '--output', help='Output EPUB file (only valid when converting a single file)')
    parser.add_argument('-ltr', '--left-to-right', action='store_true',
                       help='Use left-to-right reading direction (default is right-to-left for manga)')
    parser.add_argument('-d', '--dpi', type=int, default=150,
                       help='DPI (dots per inch) for rendering PDF pages to images (default: 150)')
    parser.add_argument('-q', '--quality', type=int, default=80,
                       help='JPEG quality (1-100, higher is better quality but larger file size, default: 80)')

    args = parser.parse_args()

    if args.output and len(args.pdf_files) > 1:
        print("Error: -o/--output can only be used when converting a single file")
        sys.exit(1)

    if args.pdf_files:
        pdf_paths = [Path(f) for f in args.pdf_files]
    else:
        pdf_paths = list(Path.cwd().glob('*.pdf'))
        pdf_paths.extend(Path.cwd().glob('*.PDF'))

    if not pdf_paths:
        print("No PDF files found")
        sys.exit(1)

    epub_output_dir = Path.cwd()
    print(f"Output directory: {epub_output_dir}")

    print(f"Converting {len(pdf_paths)} PDF file(s)")
    print(f"Reading direction: {'Left-to-Right' if args.left_to_right else 'Right-to-Left'}")

    success_count = 0
    for pdf_path in pdf_paths:
        print(f"\n{'='*60}")
        print(f"Processing: {pdf_path.name}")
        print(f"{'='*60}")

        if args.output:
            output_epub = Path(args.output)
        else:
            # Place output in EPUB directory
            epub_filename = pdf_path.stem + " (Manga).epub"
            output_epub = epub_output_dir / epub_filename

        success = convert_pdf_to_epub(pdf_path, output_epub, args.left_to_right, args.dpi, args.quality)

        if success:
            success_count += 1

    print(f"\n{'='*60}")
    print(f"Conversion complete: {success_count}/{len(pdf_paths)} successful")
    print(f"{'='*60}")

    sys.exit(0 if success_count == len(pdf_paths) else 1)


if __name__ == '__main__':
    main()
